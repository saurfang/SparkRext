---
title: "SparkRext - SparkR extension for closer to dplyr"
author: Koji MAKIYAMA (@hoxo_m)
output: 
  html_document:
    keep_md: true
---

```{r echo=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, cache=TRUE, error=TRUE)
```

```{r echo=FALSE}
knitr::knit_hooks$set(document = function(x) {
  SparkR::sparkR.stop()
  x
})
```

```{r echo=FALSE, eval=FALSE}
SparkRext::prior_package(SparkRext)
sc <- sparkR.init(master="local")
sqlContext <- sparkRSQL.init(sc)
```

## 1. Overview

[Apache Spark](https://spark.apache.org/) is one of the hottest products in data science.  
Spark 1.4.0 has formally adopted **SparkR** package which enables to handle Spark DataFrames on R.(See [this article](http://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html))

SparkR is very useful and powerful.  
One of the reasons is that SparkR DataFrames present an API similar to **dplyr**.  

For example:

```{r echo=FALSE, cache=FALSE, eval=FALSE}
library(SparkRext)
sc <- sparkR.init(master="local")
sqlContext <- sparkRSQL.init(sc)
prior_package(SparkR)
```

```{r}
df <- createDataFrame(sqlContext, iris)
df %>%
  select("Sepal_Length", "Species") %>%
  filter(df$Sepal_Length >= 5.5) %>%
  group_by(df$Species) %>%
  summarize(count=n(df$Sepal_Length), mean=mean(df$Sepal_Length)) %>%
  collect  
```

This is very cool. But I have a little discontent.

One of the reasons that dplyr is so much popular is the functions adopts NSE(non-standard evaluation).

```{r}
library(dplyr)
iris %>%
  select(Sepal.Length, Species) %>%
  filter(Sepal.Length >= 5.5) %>%
  group_by(Species) %>%
  summarize(count = n(), mean = mean(Sepal.Length))
```

It's very smart.  
With NSE, you don't need to type quotations or names of DataFrame that the columns belong to.

The package **SparkRext** have been created to make SparkR be closer to dplyr.

```{r echo=FALSE, cache=FALSE, eval=TRUE}
library(SparkRext)
prior_package(SparkRext)
```

```{r}
library(SparkRext)
df <- createDataFrame(sqlContext, iris)
df %>%
  select(Sepal_Length, Species) %>%
  filter(Sepal_Length >= 5.5) %>%
  group_by(Species) %>%
  summarize(count=n(Sepal_Length), mean=mean(Sepal_Length)) %>%
  collect  
```

SparkRext redefines the functions of SparkR to enable NSE inputs.  
As a result, the functions will be able to be used in the same way as dplyr.

## 2. How to install

The source code for SparkRext package is available on GitHub at

- https://github.com/hoxo-m/SparkRext.

You can install the package from there.

```{r eval=FALSE}
install.packages("devtools") # if you have not installed "devtools" package
devtools::install_github("hoxo-m/dplyrr")
```

## 3. Functions

SparkRext redefines six functions on SparkR.

- filter()
- select()
- mutate()
- arrange()
- summarize()
- group_by()

In this section, these funcions are explained.

For illustration, letâ€™s prepare data.

```{r}
library(dplyr)
library(nycflights13)

set.seed(123)
data <- sample_n(flights, 10000)

prior_package(SparkRext)

df <- createDataFrame(sqlContext, data.frame(data))
df %>% head
```

### 3-1. `filter()`

`filter()` is used to extract rows that the conditions specified are satisfied.

```{r}
df %>% filter(month == 12, day == 31) %>% head
```

```{r}
df %>% filter(month == 12 | day == 31) %>% head
```

Note that `filter()` of SparkR cannot accept multiple conditions at once.

### 3-2. `select()`

`select()` is used to extract columns specified.

