---
title: "SparkRext - SparkR extension for closer to dplyr"
author: Koji MAKIYAMA (@hoxo_m)
output: 
  html_document:
    keep_md: true
---

```{r echo=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, cache=TRUE, error=TRUE)
```

```{r echo=FALSE}
knitr::knit_hooks$set(document = function(x) {
  SparkR::sparkR.stop()
  x
})
```

```{r echo=FALSE, eval=FALSE}
SparkRext::prior_package(SparkRext)
sc <- sparkR.init(master="local")
sqlContext <- sparkRSQL.init(sc)
```

## 1. Overview

[Apache Spark](https://spark.apache.org/) is one of the hottest products in data science.  
Spark 1.4.0 has formally adopted **SparkR** package which enables to handle Spark DataFrames on R.(See [this article](http://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html))

SparkR is very useful and powerful.  
One of the reasons is that SparkR DataFrames present an API similar to **dplyr**.  

For example:

```{r echo=FALSE, cache=FALSE, eval=FALSE}
library(SparkRext)
sc <- sparkR.init(master="local")
sqlContext <- sparkRSQL.init(sc)
prior_package(SparkR)
```

```{r}
df <- createDataFrame(sqlContext, iris)
df %>%
  select("Sepal_Length", "Species") %>%
  filter(df$Sepal_Length >= 5.5) %>%
  group_by(df$Species) %>%
  summarize(count=n(df$Sepal_Length), mean=mean(df$Sepal_Length)) %>%
  collect  
```

This is very cool. But I have a little discontent.

One of the reasons that dplyr is so much popular is the functions adopts NSE(non-standard evaluation).

```{r}
library(dplyr)
iris %>%
  select(Sepal.Length, Species) %>%
  filter(Sepal.Length >= 5.5) %>%
  group_by(Species) %>%
  summarize(count = n(), mean = mean(Sepal.Length))
```

It's very smart.  
With NSE, you don't need to type quotations or names of DataFrame that the columns belong to.

The package **SparkRext** have been created to make SparkR be closer to dplyr.

```{r echo=FALSE, cache=FALSE, eval=TRUE}
library(SparkRext)
prior_package(SparkRext)
```

```{r}
library(SparkRext)
df <- createDataFrame(sqlContext, iris)
df %>%
  select(Sepal_Length, Species) %>%
  filter(Sepal_Length >= 5.5) %>%
  group_by(Species) %>%
  summarize(count=n(Sepal_Length), mean=mean(Sepal_Length)) %>%
  collect  
```

SparkRext redefines the functions of SparkR to enable NSE inputs.  
As a result, the functions will be able to be used in the same way as dplyr.

## 2. How to install

The source code for SparkRext package is available on GitHub at

- https://github.com/hoxo-m/SparkRext.

You can install the package from there.

```{r eval=FALSE}
install.packages("devtools") # if you have not installed "devtools" package
devtools::install_github("hoxo-m/dplyrr")
```

## 3. Functions

SparkRext redefines six functions on SparkR.

- `filter()`
- `select()`
- `mutate()`
- `arrange()`
- `summarize()`
- `group_by()`

In this section, these funcions are explained.

For illustration, letâ€™s prepare data.

```{r}
library(dplyr)
library(nycflights13)

set.seed(123)
data <- sample_n(flights, 10000)

prior_package(SparkRext)

df <- createDataFrame(sqlContext, data.frame(data))
df %>% head
```

### 3-1. `filter()`

`filter()` is used to extract rows that the conditions specified are satisfied.

```{r}
df %>% filter(month == 12, day == 31) %>% head
```

```{r}
df %>% filter(month == 12 | day == 31) %>% head
```

Note that `filter()` of SparkR cannot accept multiple conditions at once.

### 3-2. `select()`

`select()` is used to extract columns specified.

```{r}
df %>% select(year, month, day) %>% head
```

Continuous columns can be extracted using a colon `:`.

```{r}
df %>% select(year:day) %>% head
```

You can use the minus sign `-` to extract columns with the exception of columns specified.

```{r}
df %>% select(-year, -month, -day) %>% head
```

You can also extract columns by using column numbers.

```{r}
df %>% select(1, 2, 3) %>% head
```

You can use the select utility functions in dplyr such as `starts_with()`.

```{r}
df %>% select(starts_with("arr")) %>% head
```

All select utility functions is below.

- `starts_with(match, ignore.case = TRUE)`
- `ends_with(match, ignore.case = TRUE)`
- `contains(match, ignore.case = TRUE)`
- `matches(match, ignore.case = TRUE)`
- `num_range(prefix, range, width = NULL)`
- `one_of(...)`
- `everything()`

Note that select() of SparkR cannot accept a variety of input like this.

### 3-3. `mutate()`

`mutate()` is used to add new columns.

```{r}
df %>% mutate(gain = arr_delay - dep_delay, speed = distance / air_time * 60) %>% head
```

Note that `mutate()` of SparkR cannot accept multiple input at once.  
Furthermore, `mutate()` of SparkR cannot also reuse columns added like below.

```{r}
df %>% mutate(gain = arr_delay - dep_delay, gain_per_hour = gain/(air_time/60)) %>% head
```

### 3-4. `arrange()`

`arrange()` is used to sort rows by columns specified.

```{r}
df %>% arrange(month, day) %>% head
```

It will be sorted in ascending order if you write just column names.  
If you want to sort in descending order, you can use `desc()`.

```{r}
df %>% arrange(month, desc(day)) %>% head
```

You can also sort by values that are transformed from columns.

```{r}
df %>% arrange(abs(dep_delay)) %>% head
```

### 3-5. `summarize()`

`summarize()` is used to collapse a DataFrame to a single row.

```{r}
df %>% summarize(count = n(year)) %>% collect
```

Typically, `summarize()` is used with `group_by()` to collapse each group to a single row.

As far as I know, you can use the following functions in `summarize()`.

- `n()`
- `n_distinct()`
- `approxCountDistinct()`
- `mean()`
- `first()`
- `last()`

It seems that other aggregate functions are available in Scala (See [docs](http://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.sql.functions$)).

Like dplyr, you can use `summarise()` instead of `simmarize()`.

### 3-6. `group_by()`

`group_by()` is used to describe how to break a DataFrame down into groups of rows.  
Usually it is used with `summarize()` to collapse each group to a single row.

```{r}
df %>% 
  group_by(tailnum) %>%
  summarize(mean_distance = mean(distance)) %>% 
  head
```

You can indicate multiple colmuns.

```{r}
df %>% 
  group_by(year, month, day) %>%
  summarize(count = n(year)) %>% 
  arrange(year, month, day) %>%
  head
```

Unlike dplyr, only `summarize()` can receive the results of `group_by()`.

## 4. How to use

To install SparkR 1.4.0, the next articles may be useful.

- [How to use SparkR within Rstudio?](http://www.r-bloggers.com/how-to-use-sparkr-within-rstudio/)
- [SparkR with Rstudio in Ubuntu 12.04](http://www.r-bloggers.com/sparkr-with-rstudio-in-ubuntu-12-04/)

When you can load SparkR package, you will be also able to use SparkRext package.

```{r eval=FALSE}
# Load SparkRext
library(SparkRext)
prior_package(SparkRext)

# Create Spark context and SQL context
sc <- sparkR.init(master="local")
sqlContext <- sparkRSQL.init(sc)

# Preparation of data
prior_package(dplyr)
library(nycflights13)

set.seed(123)
data <- sample_n(flights, 10000)

# Create DataFrame
prior_package(SparkRext)
df <- createDataFrame(sqlContext, data.frame(data))

# Play with DataFrame
result <- df %>%
  filter(month == 12, day == 31) %>%
  mutate(gain = arr_delay - dep_delay, 
         gain_per_hour = gain/(air_time/60)) %>%
  select(tailnum, distance, gain_per_hour) %>%
  group_by(tailnum) %>%
  summarize(count = n(tailnum), 
            mean_distance = mean(distance), 
            mean_gain_per_hour = mean(gain_per_hour)) %>%
  collect

print(result)
```

```{r echo=FALSE}
# Play with DataFrame
result <- df %>%
  filter(month == 12, day == 31) %>%
  mutate(gain = arr_delay - dep_delay, 
         gain_per_hour = gain/(air_time/60)) %>%
  select(tailnum, distance, gain_per_hour) %>%
  group_by(tailnum) %>%
  summarize(count = n(tailnum), 
            mean_distance = mean(distance), 
            mean_gain_per_hour = mean(gain_per_hour)) %>%
  collect

print(result)
```

## 5. Caution points

### 5-1. `prior_package()`

SparkRext is sensitive to the order of loading of libraries.  
Thus, you should use `prior_package()` after `library()`.

```{r}
library(SparkRext)
prior_package(SparkRext)
```

By doing this, the functions of SparkRext will be called with the highest priority.  
You can confirm this by checking the search path:

```{r}
head(search())
```

If you want to switch to SparkR, you can do it.

```{r}
prior_package(SparkR)
head(search())
```

You can also switch to dplyr.

```{r}
prior_package(dplyr)
head(search())
```

### 5-2. Pipe operator `%>%`

You can use pipe operator `%>%` without loading magrittr or dplyr.  
The pipe operator imports from **pipeR** package. (See [pipeR](http://renkun.me/pipeR/))

The reason of it is that the pipe operator of pipeR is faster than magrittr.  
I will show that below.

```{r echo=FALSE}
prior_package(dplyr)
```

```{r}
library(dplyr)
library(pipeR)
library(microbenchmark)

dplyr_pipe <- function() {
  iris %>%
    select(Sepal.Length, Species) %>%
    filter(Sepal.Length >= 5.5) %>%
    group_by(Species) %>%
    summarize(count = n(), mean = mean(Sepal.Length))
}

pipeR_pipe <- function() {
  iris %>>%
    select(Sepal.Length, Species) %>>%
    filter(Sepal.Length >= 5.5) %>>%
    group_by(Species) %>>%
    summarize(count = n(), mean = mean(Sepal.Length))
}

microbenchmark(
  dplyr_pipe(),
  pipeR_pipe()
)
```

If you want to use pipe operator on the others, please overwrite it.
